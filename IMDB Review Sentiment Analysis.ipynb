{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T02:30:23.374032Z",
     "start_time": "2018-03-13T02:30:23.369412Z"
    }
   },
   "source": [
    "# IMDB Review Sentiment Analysis\n",
    "This should explain the problem with some links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:29:49.766665Z",
     "start_time": "2018-03-13T04:29:49.760708Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:29:50.493491Z",
     "start_time": "2018-03-13T04:29:50.489317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking if we have the data. If not download it\n",
    "# This needs to be done...\n",
    "# Mention that we could use the built-in keras.load_imdb() but want to do it manually for clarity\n",
    "vocab_path = 'data/imdb.vocab'\n",
    "train_path = 'data/train'\n",
    "test_path = 'data/test'\n",
    "\n",
    "VOCAB_SIZE = 20\n",
    "NUM_SAMPLES = 100\n",
    "MAX_REVIEW_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T03:18:36.790707Z",
     "start_time": "2018-03-13T03:18:36.787906Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_file_paths = glob(os.path.join(train_path, 'neg', '*'))\n",
    "# print(train_file_paths[0])\n",
    "# with open(train_file_paths[0]) as file:\n",
    "#     data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T03:18:37.099901Z",
     "start_time": "2018-03-13T03:18:37.094734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using the built-in vocab\n",
    "# with open(vocab_path) as file:\n",
    "#     vocab = file.read()\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T03:18:37.473642Z",
     "start_time": "2018-03-13T03:18:37.466087Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_sample = tokenizer.texts_to_sequences(data)\n",
    "# print(data_sample)\n",
    "\n",
    "# print(len(data))\n",
    "# print(len(data_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T03:20:15.203405Z",
     "start_time": "2018-03-13T03:20:15.201116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get an array of all the filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:29:52.885984Z",
     "start_time": "2018-03-13T04:29:52.821710Z"
    }
   },
   "outputs": [],
   "source": [
    "train_files = {}\n",
    "train_files['pos'] = glob(os.path.join(train_path, 'pos', '*.txt'))\n",
    "train_files['neg'] = glob(os.path.join(train_path, 'neg', '*.txt'))\n",
    "# train_files['unsup'] = glob(os.path.join(train_path, 'unsup', '*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to get the indices for training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:29:54.156930Z",
     "start_time": "2018-03-13T04:29:54.141754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "200 200\n"
     ]
    }
   ],
   "source": [
    "# Turn this into a function at some point soon so we can get train, test, val\n",
    "# This might take a while, needs to read in a large amount of data\n",
    "# Later read in the testing data as well (write a function)\n",
    "x_train = []\n",
    "y_train = []\n",
    "sentiment_map = {'pos': 1, 'neg': 0}\n",
    "for sentiment in train_files:\n",
    "    print(len(train_files[sentiment]))\n",
    "    # Do the shuffle here for indices\n",
    "    # Try different splits\n",
    "    for file_name in train_files[sentiment][0:NUM_SAMPLES]:\n",
    "        temp_ = []\n",
    "        with open(file_name) as file_:\n",
    "            temp_ = file_.read()\n",
    "        x_train.append(temp_)\n",
    "        y_train.append(sentiment_map[sentiment])\n",
    "\n",
    "print(len(x_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:29:56.395246Z",
     "start_time": "2018-03-13T04:29:56.339670Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_REVIEW_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:20:31.951601Z",
     "start_time": "2018-03-13T04:20:31.748697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 32)            640       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 53,941\n",
      "Trainable params: 53,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "\n",
    "# Look into these hyper-parameters\n",
    "# eventually will use a sweep\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, embedding_vecor_length, input_length=MAX_REVIEW_LEN))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:20:37.308685Z",
     "start_time": "2018-03-13T04:20:35.913658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/Development/personal/sentiment_analysis/venv/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6935 - acc: 0.4850\n",
      "Epoch 2/3\n",
      "200/200 [==============================] - 0s 546us/step - loss: 0.6923 - acc: 0.5250\n",
      "Epoch 3/3\n",
      "200/200 [==============================] - 0s 595us/step - loss: 0.6917 - acc: 0.5850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa6f9623a20>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, nb_epoch=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:28:41.860255Z",
     "start_time": "2018-03-13T04:28:41.857789Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_test, y_test = get_encoded_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:28:39.902762Z",
     "start_time": "2018-03-13T04:28:39.900379Z"
    }
   },
   "outputs": [],
   "source": [
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
